{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"colab":{"name":"M2_SNB_MiniProject_2_Credit_risk_modelling_Logistic_Regression.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"associate-sunset"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Mini-Project: Credit risk modelling using Logistic Regression"],"id":"associate-sunset"},{"cell_type":"markdown","metadata":{"id":"n-kxaHhwXEp9"},"source":["**DISCLAIMER:** THIS NOTEBOOK IS PROVIDED ONLY AS A REFERENCE SOLUTION NOTEBOOK FOR THE MINI-PROJECT. THERE MAY BE OTHER POSSIBLE APPROACHES/METHODS TO ACHIEVE THE SAME RESULTS."],"id":"n-kxaHhwXEp9"},{"cell_type":"markdown","metadata":{"id":"handled-tooth"},"source":["## Problem Statement"],"id":"handled-tooth"},{"cell_type":"markdown","metadata":{"id":"accessory-watts"},"source":["Predict the loan defaulters using a Logistic Regression model on the credit risk data and calculate credit scores"],"id":"accessory-watts"},{"cell_type":"markdown","metadata":{"id":"twenty-indonesia"},"source":["## Learning Objectives"],"id":"twenty-indonesia"},{"cell_type":"markdown","metadata":{"id":"honest-friendship"},"source":["At the end of the mini-project, you will be able to :\n","\n","* perform data exploration, preprocessing and visualization\n","* implement Logistic Regression using manual code or using sklearn library\n","* evaluate the model using appropriate performance metrics\n","* develop a credit scoring system"],"id":"honest-friendship"},{"cell_type":"markdown","metadata":{"id":"lesbian-bottom"},"source":["## Dataset"],"id":"lesbian-bottom"},{"cell_type":"markdown","metadata":{"id":"fixed-trainer"},"source":["The dataset chosen for this mini-project is the [Give Me Some Credit](https://cdn.iisc.talentsprint.com/CDS/Give_me_some_credit_BigML.pdf) dataset which can be used to build models for predicting loan repayment defaulters\n","\n","#### Datafields\n","\n","- **SeriousDlqin2yrs:** Person experienced 90 days past due delinquency or worse\n","- **RevolvingUtilizationOfUnsecuredLines:** Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits\n","- **age:** Age of borrower in years\n","- **NumberOfTime30-59DaysPastDueNotWorse:** Number of times borrower has been 30-59 days past due but no worse in the last 2 years.\n","- **DebtRatio:** Monthly debt payments, alimony,living costs divided by monthy gross income\n","- **MonthlyIncome:** Monthly income\n","- **NumberOfOpenCreditLinesAndLoans:** Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards)\n","- **NumberOfTimes90DaysLate:** Number of times borrower has been 90 days or more past due.\n","- **NumberRealEstateLoansOrLines:**\tNumber of mortgage and real estate loans including home equity lines of credit\n","- **NumberOfTime60-89DaysPastDueNotWorse:**\tNumber of times borrower has been 60-89 days past due but no worse in the last 2 years.\n","- **NumberOfDependents:** Number of dependents in family excluding themselves (spouse, children etc.)"],"id":"fixed-trainer"},{"cell_type":"markdown","metadata":{"id":"rapid-hierarchy"},"source":["## Information"],"id":"rapid-hierarchy"},{"cell_type":"markdown","metadata":{"id":"prescribed-matter"},"source":["Credit risk arises when a corporate or individual borrower fails to meet their debt obligations. From the lender's perspective, credit risk could disrupt its cash flows or increase collection costs, since the lender may be forced to hire a debt collection agency to enforce the collection. The loss may be partial or complete, where the lender incurs a loss of part of the loan or the entire loan extended to the borrower.\n","\n","Credit scoring algorithms, which calculate the probability of default, are the best methods that banks use to determine whether or not a loan should be granted. \n","\n","In order to build a credit scoring system, the following feature transformations are performed:\n","\n","#### Weight of Evidence and Information value\n","\n","Logistic regression is a commonly used technique in credit scoring for solving binary classification problems. Prior to model fitting, another iteration of variable selection is valuable to check if the newly WOE transformed variables are still good model candidates. Preferred candidate variables are those with higher information value having a linear relationship with the dependent variable, have good coverage across all categories, have a normal distribution, contain a notable overall contribution, and are relevant to the business.\n","\n","**Weight of evidence** (WOE) is a powerful tool for feature representation and evaluation in data science. WOE can provide interpret able transformation to both categorical and numerical features. The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. Since it evolved from credit scoring world, it is generally described as a measure of the separation of good and bad customers. \"Bad Customers\" refers to the customers who defaulted on a loan. and \"Good Customers\" refers to the customers who paid back loan. WOE can be calculated using the below formula:\n","\n","$$WOE = ln \\left( \\frac{\\%   of  Non\\_Events}{\\%   of  Events} \\right)$$\n","\n","Steps to calculate WOE\n","* For a continuous variable, split data into 10 parts (or lesser depending on the distribution).\n","* Calculate the number of events and non-events in each group (bin)\n","* Calculate the % of events and % of non-events in each group.\n","* Calculate WOE by taking natural log of division of % of non-events and % of events\n","\n","**Information value** is one of the most useful technique to select important variables in a predictive model. It helps to rank variables on the basis of their importance. The IV is calculated using the following formula :\n","$$IV = ∑ (\\% of Non\\_Events - \\% of Events) * WOE$$\n","\n","Read more about `WOE` and `IV` from the following [link](https://medium.com/@yanhuiliu104/credit-scoring-scorecard-development-process-8554c3492b2b)"],"id":"prescribed-matter"},{"cell_type":"markdown","metadata":{"id":"operating-latter"},"source":["## Grading = 10 Points"],"id":"operating-latter"},{"cell_type":"markdown","metadata":{"id":"caring-syndrome"},"source":["### Download the dataset"],"id":"caring-syndrome"},{"cell_type":"code","metadata":{"id":"comparable-delay","cellView":"form"},"source":["#@title Download dataset\n","!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/GiveMeSomeCredit.csv\n","!pip -qq install xverse"],"id":"comparable-delay","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"appreciated-pattern"},"source":["### Import Neccesary Packages"],"id":"appreciated-pattern"},{"cell_type":"code","metadata":{"id":"loose-marsh"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","import math\n","from xverse.transformer import MonotonicBinning,WOE\n","%matplotlib inline"],"id":"loose-marsh","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"compressed-reflection"},"source":["### Load the dataset"],"id":"compressed-reflection"},{"cell_type":"code","metadata":{"id":"fatty-graph"},"source":["train_data = pd.read_csv(\"GiveMeSomeCredit.csv\")\n","train_data.head(2)"],"id":"fatty-graph","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"experienced-sleeping"},"source":["#### Describe the all statistical properties of the train dataset"],"id":"experienced-sleeping"},{"cell_type":"code","metadata":{"id":"greek-methodology"},"source":["train_data[train_data.columns[1:]].describe()"],"id":"greek-methodology","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"christian-hamilton"},"source":["### Pre-processing (1 point)"],"id":"christian-hamilton"},{"cell_type":"markdown","metadata":{"id":"global-decision"},"source":["#### Remove unwanted columns"],"id":"global-decision"},{"cell_type":"code","metadata":{"id":"pharmaceutical-latvia"},"source":["train_data.drop(\"Unnamed: 0\",axis=1,inplace=True)"],"id":"pharmaceutical-latvia","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usual-elimination"},"source":["#### Handle the missing data\n","\n","Find the how many null values in the dataset and fill with mean or remove."],"id":"usual-elimination"},{"cell_type":"code","metadata":{"id":"standing-trinity"},"source":["train_data.isna().sum()"],"id":"standing-trinity","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"heated-findings"},"source":["# Fill the missing values using mean\n","train_data= train_data.fillna((train_data.mean()))\n","train_data.isna().sum()"],"id":"heated-findings","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hispanic-reply"},"source":["### EDA &  Visualization ( 1 point)"],"id":"hispanic-reply"},{"cell_type":"markdown","metadata":{"id":"standing-cheese"},"source":["#### Calculate the percentage of the target lebels and visualize with a graph"],"id":"standing-cheese"},{"cell_type":"code","metadata":{"id":"attractive-hands"},"source":["total_len = len(train_data['SeriousDlqin2yrs'])\n","percentage_labels = (train_data['SeriousDlqin2yrs'].value_counts()/total_len)*100\n","percentage_labels"],"id":"attractive-hands","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cutting-citizenship"},"source":["sns.countplot(train_data.SeriousDlqin2yrs).set_title('Data Distribution')\n","ax = plt.gca()\n","for p in ax.patches:\n","    height = p.get_height()\n","    ax.text(p.get_x() + p.get_width()/2., height + 2, '{:.2f}%'.format(100*(height/total_len)), fontsize=14, ha='center', va='bottom')\n","ax.set_xlabel(\"Labels for SeriousDlqin2yrs attribute\")\n","ax.set_ylabel(\"Numbers of records\")\n","plt.show()"],"id":"cutting-citizenship","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"satisfactory-stopping"},"source":["#### Plot the distribution of SeriousDlqin2yrs by age"],"id":"satisfactory-stopping"},{"cell_type":"code","metadata":{"id":"multiple-series"},"source":["sns.kdeplot(train_data[train_data[\"SeriousDlqin2yrs\"] == 0][\"age\"], label=\"Not in 2 years\")\n","sns.kdeplot(train_data[train_data[\"SeriousDlqin2yrs\"] == 1][\"age\"], label=\"In 2 years\")\n","plt.xlabel('Age')\n","plt.title('Distribuition of Default Rate by Age')\n","plt.show()"],"id":"multiple-series","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"promotional-rolling"},"source":["#### Correlation and the heatmap"],"id":"promotional-rolling"},{"cell_type":"code","metadata":{"id":"studied-candidate"},"source":["train_data[train_data.columns[:]].corr()\n","sns.heatmap(train_data[train_data.columns[:]].corr(),fmt=\".1f\")\n","plt.show()"],"id":"studied-candidate","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"operational-minute"},"source":["### Data Engineering (2 points)"],"id":"operational-minute"},{"cell_type":"markdown","metadata":{"id":"outer-telephone"},"source":["#### Weight of Evidence and Information value\n","\n","* Arrange the binning for each variable with different bins\n","    * For eg. Age = 49, Age_quantile_range = (48, 56)\n","* Calculate information value and chooose the best features based on the rules given below\n","\n","| Information Value |\tVariable Predictiveness |\n","| --- | --- |\n","| Less than 0.02\t|  Not useful for prediction |\n","| 0.02 to 0.1\t| Weak predictive Power |\n","|  0.1 to 0.3 | Medium predictive Power |\n","| 0.3 to 0.5 | Strong predictive Power |\n","| >0.5 | Suspicious Predictive Power |\n","\n","* Calculate Weight of evidence for the selected variables\n","\n","Hint: Use [xverse](https://towardsdatascience.com/introducing-xverse-a-python-package-for-feature-selection-and-transformation-17193cdcd067). It is a machine learning Python module in the space of feature engineering, feature transformation and feature selection. It provides pre-built functions for the above steps, such as binning and conversion to WoE."],"id":"outer-telephone"},{"cell_type":"code","metadata":{"id":"norwegian-telescope"},"source":["# Using xverse package\n","clf = MonotonicBinning()\n","clf.fit(train_data.iloc[:,1:], train_data.iloc[:,0])\n","out_X = clf.transform(train_data.iloc[:,1:])\n","out_X.head()"],"id":"norwegian-telescope","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"liked-satin"},"source":["# Using xverse package\n","clf = WOE()\n","clf.fit(train_data.iloc[:,1:], train_data.iloc[:,0])\n","out_X = clf.transform(train_data.iloc[:,1:])"],"id":"liked-satin","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"charitable-anxiety"},"source":["clf.iv_df #information value dataset"],"id":"charitable-anxiety","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ordered-knock"},"source":["selected_columns = clf.iv_df.Variable_Name.values[:5]"],"id":"ordered-knock","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"conservative-rebel"},"source":["### Identify features,  target and split it into train and test"],"id":"conservative-rebel"},{"cell_type":"code","metadata":{"id":"ambient-dress"},"source":["len(out_X.columns), len(selected_columns)"],"id":"ambient-dress","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5hoEeUh66UeT"},"source":["train_data.iloc[:,1:]"],"id":"5hoEeUh66UeT","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"harmful-warrant"},"source":["X = out_X[selected_columns]\n","y = train_data['SeriousDlqin2yrs']\n","X.shape, y.shape"],"id":"harmful-warrant","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"packed-humor"},"source":["# split the data into train and test\n","xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=66)\n","xtrain.shape, ytrain.shape, xtest.shape, ytest.shape"],"id":"packed-humor","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ufESq5OxgoXa"},"source":["### Logistic Regression from scratch using gradient method (2 points)\n","\n","For Linear Regression, we had the hypothesis $yhat = w.X +b$ , whose output range was the set of all Real Numbers.\n","Now, for Logistic Regression our hypothesis is  $yhat = sigmoid(w.X + b)$ , whose output range is between 0 and 1 because by applying a sigmoid function, we always output a number between 0 and 1.\n","\n","$yhat = \\frac{1}{1 +e^{-(w.x+b)}}$\n","\n","Hint: [logistic-regression-with-python](\n","https://medium.com/@ODSC/logistic-regression-with-python-ede39f8573c7)"],"id":"ufESq5OxgoXa"},{"cell_type":"code","metadata":{"id":"precious-business"},"source":["intercept = np.ones((xtrain.shape[0], 1))  \n","x_train = np.concatenate((intercept, xtrain), axis=1)\n","weight = np.zeros(x_train.shape[1])"],"id":"precious-business","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"confidential-curtis"},"source":["weight.shape"],"id":"confidential-curtis","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"middle-mailing"},"source":["#Sigmoid method\n","def sigmoid(x, weight):\n","    z = np.dot(x, weight)\n","    return 1 / (1 + np.exp(-z))\n","\n","from sklearn.metrics import mean_squared_error\n","\n","def fit(x, y, weight, lr, iterations):\n","    for i in range(iterations):\n","        sigma = sigmoid(x, weight)\n","        loss = mean_squared_error(sigma,y)\n","        # gradient\n","        dW = np.dot(x.T, (sigma - y)) / y.shape[0]\n","        #Updating the weights\n","        weight -= lr * dW\n","    return weight"],"id":"middle-mailing","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"quick-study"},"source":["#creating the class Object\n","updated_weights = fit(x_train, ytrain, weight, 0.1 , 5000)"],"id":"quick-study","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vertical-layout"},"source":["# Method to predict the class label.\n","def predict(x_new , weight, treshold):\n","    x_new = np.concatenate((np.ones((x_new.shape[0], 1)), x_new), axis=1)  \n","    result = sigmoid(x_new, weight)\n","    result = result >= treshold\n","    y_pred = np.zeros(result.shape[0])\n","    for i in range(len(y_pred)):\n","        if result[i] == True: \n","            y_pred[i] = 1\n","        else:\n","            continue\n","    return y_pred"],"id":"vertical-layout","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tracked-intent"},"source":["xtest.shape"],"id":"tracked-intent","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bigger-guidance"},"source":["y_pred = predict(xtest, updated_weights, 0.5)"],"id":"bigger-guidance","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lasting-sector"},"source":["# Accuracy of test data\n","(y_pred == ytest).sum() / len(y_pred)"],"id":"lasting-sector","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7eSQ5nuBgk7S"},"source":["### Implement the Logistic regression using sklearn (2 points)\n","\n","As there is imbalance in the class distribution, add weightage to the Logistic regression.\n","\n","* Find the accuracy with class weightage in Logistic regression\n","* Find the accuracy without class weightage in Logistic regression\n","\n","Hint: [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"],"id":"7eSQ5nuBgk7S"},{"cell_type":"code","metadata":{"id":"impressive-assistant"},"source":["# With weightage\n","log_reg = LogisticRegression(class_weight={0:6/100, 1: 94/100})\n","log_reg.fit(xtrain,ytrain)\n","log_reg.score(xtest,ytest), log_reg.score(xtrain, ytrain)"],"id":"impressive-assistant","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AFz6VF-m5roR"},"source":["test_predicted = log_reg.predict(xtest)"],"id":"AFz6VF-m5roR","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"similar-flower"},"source":["# Without weightage\n","log_reg = LogisticRegression()\n","log_reg.fit(xtrain,ytrain)\n","log_reg.score(xtest,ytest), log_reg.score(xtrain, ytrain)"],"id":"similar-flower","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fossil-washer"},"source":["test_predicted = log_reg.predict(xtest)"],"id":"fossil-washer","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"heard-briefing"},"source":["log_reg.coef_, log_reg.intercept_"],"id":"heard-briefing","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q68_0W3G9jCn"},"source":["log_reg.predict_proba"],"id":"q68_0W3G9jCn","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"signal-error"},"source":["### Credit scoring (1 point)\n","\n","When scaling the model into a scorecard, we will need both the Logistic Regression coefficients from model fitting as well as the transformed WoE values. We will also need to convert the score from the model from the log-odds unit to a points system.\n","For each independent variable Xi, its corresponding score is:\n","\n","$Score = \\sum_{i=1}^{n} (-(β_i × WoE_i + \\frac{α}{n}) × Factor + \\frac{Offset}{n})$\n","\n","Where:\n","\n","βi — logistic regression coefficient for the variable Xi\n","\n","α — logistic regression intercept\n","\n","WoE — Weight of Evidence value for variable Xi\n","\n","n — number of independent variable Xi in the model\n","\n","Factor, Offset — known as scaling parameter\n","\n","  - Factor = pdo / ln(2); pdo is points to double the odds\n","  - Offset = Round_of_Score - {Factor * ln(Odds)}"],"id":"signal-error"},{"cell_type":"code","metadata":{"id":"worst-spare"},"source":["coef = log_reg.coef_.ravel()\n","intercept = log_reg.intercept_\n","factor = 20/np.log(2)\n","offset = 600 - ( factor * np.log(50))\n","factor, offset"],"id":"worst-spare","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MwSXbZ6_P8Jr"},"source":["# 1st method\n","# all_scores = []\n","# for idx,row in X.iterrows():\n","#   score  = []\n","#   for j in range(len(row)):\n","#     asum = (-((row[j] * coef[j]) + (intercept/X.shape[1])) * factor) + (offset/X.shape[1])\n","#     score.append(asum)\n","#   all_scores.append(sum(score))\n","# max(all_scores), min(all_scores)"],"id":"MwSXbZ6_P8Jr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XwwnwQKMU_Nx"},"source":["# 2nd method\n","all_scores = []\n","for idx,row in X.iterrows():\n","  a = row.values * coef          # B_i * WOE_i\n","  a = a + (intercept/X.shape[1]) # (B_i * WOE_i) + intercept_i / n\n","  b = -a * factor                # -((B_i * WOE_i) + intercept_i / n) * factor\n","  b = b + (offset/X.shape[1])    # -((B_i * WOE_i) + intercept_i / n) * factor) + offset / n\n","  all_scores.append(sum(b))      # sum"],"id":"XwwnwQKMU_Nx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vOcB1ewAYxtM"},"source":["max(all_scores),min(all_scores)"],"id":"vOcB1ewAYxtM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.array(all_scores)"],"metadata":{"id":"sjs0UE90q3WR"},"id":"sjs0UE90q3WR","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"intelligent-internship"},"source":["### Performance Metrics (1 point)"],"id":"intelligent-internship"},{"cell_type":"markdown","metadata":{"id":"innocent-hygiene"},"source":["#### Precision"],"id":"innocent-hygiene"},{"cell_type":"code","metadata":{"id":"optimum-listening"},"source":["from sklearn.metrics import precision_score\n","precision_score(ytest, test_predicted ,average='macro') "],"id":"optimum-listening","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"accessory-keyboard"},"source":["#### Recall"],"id":"accessory-keyboard"},{"cell_type":"code","metadata":{"id":"civic-corner"},"source":["from sklearn.metrics import recall_score\n","recall_score(ytest, test_predicted,average='macro') "],"id":"civic-corner","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wired-amendment"},"source":["#### Classification Report"],"id":"wired-amendment"},{"cell_type":"code","metadata":{"id":"impossible-machinery"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(ytest, test_predicted))"],"id":"impossible-machinery","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dense-feelings"},"source":["#### Confusion matrix"],"id":"dense-feelings"},{"cell_type":"code","metadata":{"id":"running-remains"},"source":["from sklearn.metrics import confusion_matrix\n","mat = confusion_matrix(ytest, test_predicted)\n","mat"],"id":"running-remains","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## SHAP implementation for Logistic Regression"],"metadata":{"id":"RG1rMiEjvdWG"},"id":"RG1rMiEjvdWG"},{"cell_type":"markdown","source":["**SHAP (SHapley Additive exPlanations)** is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions.\n","\n","In the below figure, we can see how the different feature values (Age, Sex, BMI, etc) are affecting the base value (0.1) to give the final output prediction (0.4). The base value or the expected value is the average of the model output over the training data X_train.\n","\n","![](https://cdn.iisc.talentsprint.com/CDS/Images/Shap_model.png)"],"metadata":{"id":"8krue0HcMDMW"},"id":"8krue0HcMDMW"},{"cell_type":"markdown","source":["To understand how to compute and interpet Shapley-based explanations of a machine learning model, we will use the following plots:\n","\n","- Force plot\n","- Feature importance plot\n","- Summary plot\n","- Dependence plot\n","- Clustering Shapley values"],"metadata":{"id":"x68S9Hw1QLx5"},"id":"x68S9Hw1QLx5"},{"cell_type":"code","source":["!pip -qq install shap"],"metadata":{"id":"yW5YV_hFvhW1"},"execution_count":null,"outputs":[],"id":"yW5YV_hFvhW1"},{"cell_type":"code","source":["import shap\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"8b-jRrKMvfvi"},"execution_count":null,"outputs":[],"id":"8b-jRrKMvfvi"},{"cell_type":"markdown","source":["### Force plot for individual instances\n","\n","We can visualize feature attributions such as Shapley values as “forces”. Each feature value is a force that either increases or decreases the prediction. The prediction starts from the baseline. The baseline for Shapley values is the average of all predictions. \n","\n","In the plot, each Shapley value is an arrow that pushes to increase (positive value) or decrease (negative value) the prediction. These forces balance each other out at the actual prediction of the data instance.\n","\n","Here we use `shap.force_plot()` function that takes three values: \n","\n","- the base value (explainer.expected_value), \n","- the SHAP values, and \n","- the matrix of feature values \n","\n","Let's see SHAP force plots for two instances from the GiveMeSomeCredit dataset:"],"metadata":{"id":"XUKDBsERsyY3"},"id":"XUKDBsERsyY3"},{"cell_type":"code","source":["# Instead of using the whole training set to estimate expected values, we summarize with\n","# a set of weighted kmeans, each weighted by the number of points they represent.\n","xtrain_summary = shap.kmeans(xtrain, 10)      # summarize the background as K samples"],"metadata":{"id":"t8j3OnI8xi2Z"},"execution_count":null,"outputs":[],"id":"t8j3OnI8xi2Z"},{"cell_type":"code","source":["feature_names = selected_columns.tolist()"],"metadata":{"id":"eoXN6P3H5D9q"},"execution_count":null,"outputs":[],"id":"eoXN6P3H5D9q"},{"cell_type":"code","source":["# Force plot for an instance\n","shap.initjs()\n","explainer = shap.KernelExplainer(log_reg.predict, xtrain_summary)\n","shap_value = explainer.shap_values(xtest.iloc[0,:].values)\n","shap.force_plot(explainer.expected_value, shap_value, xtest.iloc[0,:].values, feature_names = feature_names)"],"metadata":{"id":"r0I7-Qfpv8uW"},"execution_count":null,"outputs":[],"id":"r0I7-Qfpv8uW"},{"cell_type":"markdown","source":["In the above plot:\n","\n","- The output value **f(x)** is the prediction for that observation (the predicted output value of the first row in X_test is ≈ **0**, indicating low credit risk).\n","- The **base value**: is “the value that would be predicted if we did not know any features for the current output.” Here it is **0.004808**.\n","- Red/blue: Features that push the prediction value higher (to the right) are shown in red, and those pushing the prediction value lower are in blue.\n","\n","The features are having risk decreasing effects."],"metadata":{"id":"sRETKd6FpdKI"},"id":"sRETKd6FpdKI"},{"cell_type":"code","source":["# Force plot for another instance\n","shap.initjs()\n","explainer = shap.KernelExplainer(log_reg.predict, xtrain_summary)\n","shap_value = explainer.shap_values(xtest.iloc[1,:].values)\n","shap.force_plot(explainer.expected_value, shap_value, xtest.iloc[1,:].values, feature_names = feature_names)"],"metadata":{"id":"vkKVLz8r-3p7"},"execution_count":null,"outputs":[],"id":"vkKVLz8r-3p7"},{"cell_type":"markdown","source":["In the above plot, the output value **f(x)** i.e, the predicted output value for that observation is ≈ **0**.\n","\n","This instance also has a low predicted risk of 0. But the risk decreasing effects such as `NumberOfTimes90DaysLate` are offset by increasing effects such as `NumberOfTime30-59DaysPastDueNotWorse`."],"metadata":{"id":"wW3cmp2ntmmp"},"id":"wW3cmp2ntmmp"},{"cell_type":"markdown","source":["Shapley values can be combined into global explanations. If we run SHAP for every instance, we get a matrix of Shapley values. This matrix has one row per data instance and one column per feature. We can interpret the entire model by analyzing the Shapley values in this matrix."],"metadata":{"id":"ln9LdWyIZc6l"},"id":"ln9LdWyIZc6l"},{"cell_type":"markdown","source":["### SHAP Feature Importance\n","\n","The idea behind SHAP feature importance is simple: Features with large **absolute** Shapley values are important. Since we want the global importance, we average the absolute Shapley values per feature across the data. Next, we sort the features by decreasing importance and plot them. \n","\n","Let's plot the SHAP feature importance for the logistic regression model trained before for predicting credit risk."],"metadata":{"id":"UIe6Cknit57j"},"id":"UIe6Cknit57j"},{"cell_type":"markdown","source":["Here, we use the `shap.summary_plot` function with `plot_type=”bar”` to produce the feature importance plot. It lists the most significant features in descending order. The top variables contribute more to the model than the bottom ones and thus have high predictive power."],"metadata":{"id":"M1CC9wJGVs36"},"id":"M1CC9wJGVs36"},{"cell_type":"code","source":["# Get SHAP values\n","shap_values = explainer.shap_values(xtest)"],"metadata":{"id":"bqu2pPn9nu-d"},"execution_count":null,"outputs":[],"id":"bqu2pPn9nu-d"},{"cell_type":"code","source":["# Feature importance\n","shap.summary_plot(shap_values, xtest, feature_names = feature_names, plot_type = \"bar\")"],"metadata":{"id":"jfPzt7DViLKB"},"execution_count":null,"outputs":[],"id":"jfPzt7DViLKB"},{"cell_type":"markdown","source":["For a more informative plot, we will look at the summary plot."],"metadata":{"id":"il3Uuj3PuQY3"},"id":"il3Uuj3PuQY3"},{"cell_type":"markdown","source":["### SHAP Summary Plot\n","\n","The summary plot combines feature importance with feature effects:\n","\n","* Each point on the summary plot is a Shapley value for a feature and an instance. \n","* The position on the y-axis is determined by the feature and on the x-axis by the Shapley value. \n","* Overlapping points are jittered in y-axis direction, so we get a sense of the distribution of the Shapley values per feature. "],"metadata":{"id":"f9AmqiJCuTr3"},"id":"f9AmqiJCuTr3"},{"cell_type":"markdown","source":["It demonstrates the following information:\n","\n","- **Feature importance**: Features are ranked in descending order.\n","- **Impact**: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n","- **Original value**: Color shows whether that feature value is high (in red) or low (in blue) for that observation.\n","- **Correlation**: A *high* level of the “NumberOfTimes90DaysLate” has a *positive* impact on the credit risk. The “high” comes from the red color, and the “positive” impact is shown on the X-axis. Similarly, we will say the “NumberOfTime30-59DaysPastDueNotWorse” is also positively correlated with the target variable."],"metadata":{"id":"wHI5TIP8hF8O"},"id":"wHI5TIP8hF8O"},{"cell_type":"code","source":["# Summary plot\n","shap.summary_plot(shap_values, xtest, feature_names = feature_names, plot_type = \"dot\")"],"metadata":{"id":"1JocSmx8-lo9"},"execution_count":null,"outputs":[],"id":"1JocSmx8-lo9"},{"cell_type":"markdown","source":["In the summary plot, we see first indications of the relationship between the value of a feature and the impact on the prediction. But to see the exact form of the relationship, we have to look at SHAP dependence plots."],"metadata":{"id":"FtZmi2dEuc9n"},"id":"FtZmi2dEuc9n"},{"cell_type":"markdown","source":["### SHAP Dependence Plot"],"metadata":{"id":"7oGleQqto1kN"},"id":"7oGleQqto1kN"},{"cell_type":"markdown","source":["The partial dependence plot shows the marginal effect of one or two features have on the predicted outcome of a machine learning model. It tells whether the relationship between the target and a feature is linear, monotonic or more complex.\n","\n","To implement SHAP feature dependence plot: \n","\n","* Pick a feature\n","* For each data instance, plot a point with the feature value on the x-axis and the corresponding Shapley value on the y-axis\n","\n","In order to create a dependence plot, we use `shap.dependence_plot()` function. The function automatically includes another variable that the chosen variable interacts most with. The following plot shows the relationship between “age” and the target variable, and “age” interacts with “NumberOfTime60-89DaysPastDueNotWorse” feature frequently."],"metadata":{"id":"g5NhkzG3jOoY"},"id":"g5NhkzG3jOoY"},{"cell_type":"code","source":["# Dependence plot for temperature\n","shap.dependence_plot(\"age\", shap_values, xtest)"],"metadata":{"id":"A7kKZtGS-1Kf"},"execution_count":null,"outputs":[],"id":"A7kKZtGS-1Kf"},{"cell_type":"markdown","source":["### Clustering Shapley Values\n","\n","We can cluster the data with the help of Shapley values. The goal of clustering is to find groups of similar instances. \n","\n","SHAP clustering works by clustering the Shapley values of each instance. This means that we cluster instances by explanation similarity. All SHAP values have the same unit – the unit of the prediction space. We can use any clustering method. The following example uses hierarchical agglomerative clustering to order the instances.\n","\n","The plot consists of many force plots, each of which explains the prediction of an instance. We rotate the force plots vertically and place them side by side according to their clustering similarity."],"metadata":{"id":"hM7Zmir4vbAq"},"id":"hM7Zmir4vbAq"},{"cell_type":"code","source":["# Force plot for first 100 instances of xtest\n","shap.initjs()\n","shap.force_plot(explainer.expected_value, shap_values[0:200], xtest.iloc[0:200], feature_names = feature_names)"],"metadata":{"id":"Dr_xw8XQeSj3"},"execution_count":null,"outputs":[],"id":"Dr_xw8XQeSj3"},{"cell_type":"markdown","source":["In the above plot, each position on the x-axis is an instance of the data. Red SHAP values increases the prediction whereas blue values decreases it.\n","\n","Here, we have used only 200 instances and we see one cluster stands out: On the right is a group with a high predicted credit risk.\n","\n","Also, we can plot it for the entire set using the below code cell. Note that it may take more than 30 minutes to run the below cell."],"metadata":{"id":"-jUbybsjvl11"},"id":"-jUbybsjvl11"},{"cell_type":"code","source":["# Force plot for entire xtest\n","#shap.initjs()\n","#shap.force_plot(explainer.expected_value, shap_values, xtest.values, feature_names = feature_names)"],"metadata":{"id":"loLIovJx4lUi"},"id":"loLIovJx4lUi","execution_count":null,"outputs":[]}]}