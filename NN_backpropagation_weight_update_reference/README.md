# ğŸ§  Neural Network Backpropagation & Weight Update â€” Quick Reference

This repository provides a concise overview of **Neural Network Backpropagation**, including the key equations, steps, and update rules for training feedforward networks.

---

## ğŸ“˜ Overview

**Backpropagation** is the algorithm used to train neural networks by minimizing the **loss function** through **gradient descent**.  
It propagates the error backward from the output layer to the input layer and updates weights to reduce prediction error.

---

## ğŸ§© Key Steps in Backpropagation

### 1. Forward Pass
Compute the output of each neuron layer by layer.

**Equations:**
- <b>z<sup>[l]</sup> = W<sup>[l]</sup> a<sup>[lâˆ’1]</sup> + b<sup>[l]</sup></b>  
- <b>a<sup>[l]</sup> = f(z<sup>[l]</sup>)</b>

---

### 2. Compute Loss
Example (Mean Squared Error):

<b>L = (1 / 2m) Î£<sub>i</sub> (y<sub>i</sub> âˆ’ Å·<sub>i</sub>)Â²</b>

---

### 3. Backward Pass (Error Propagation)

Compute gradients layer-by-layer from the output backward:

- <b>Î´<sup>[L]</sup> = (Å· âˆ’ y) âŠ™ fâ€²(z<sup>[L]</sup>)</b>  
- <b>Î´<sup>[l]</sup> = (W<sup>[l+1]</sup>)áµ€ Î´<sup>[l+1]</sup> âŠ™ fâ€²(z<sup>[l]</sup>)</b>

---

### 4. Compute Gradients

- <b>âˆ‚L/âˆ‚W<sup>[l]</sup> = (1/m) Â· Î´<sup>[l]</sup> (a<sup>[lâˆ’1]</sup>)áµ€</b>  
- <b>âˆ‚L/âˆ‚b<sup>[l]</sup> = (1/m) Î£ Î´<sup>[l]</sup></b>

---

### 5. Weight & Bias Update

- <b>W<sup>[l]</sup> â† W<sup>[l]</sup> âˆ’ Î· Â· âˆ‚L/âˆ‚W<sup>[l]</sup></b>  
- <b>b<sup>[l]</sup> â† b<sup>[l]</sup> âˆ’ Î· Â· âˆ‚L/âˆ‚b<sup>[l]</sup></b>

Where:
- Î· = learning rate  
- âˆ‚L/âˆ‚W, âˆ‚L/âˆ‚b = gradients

---

## âš™ï¸ Simplified Example (Single Neuron)

Given:  
<b>y = f(Wx + b)</b>

Loss:  
<b>L = Â½ (y<sub>true</sub> âˆ’ y)Â²</b>

Updates:  
- <b>dL/dW = (y âˆ’ y<sub>true</sub>) Â· fâ€²(z) Â· x</b>  
- <b>W â† W âˆ’ Î· (dL/dW)</b>  
- <b>b â† b âˆ’ Î· (y âˆ’ y<sub>true</sub>) Â· fâ€²(z)</b>

---

## ğŸ§® Activation Functions & Derivatives

| Function | f(x) | fâ€²(x) |
|-----------|-------|-------|
| **Sigmoid** | 1 / (1 + e<sup>âˆ’x</sup>) | f(x)(1 âˆ’ f(x)) |
| **Tanh** | tanh(x) | 1 âˆ’ f(x)Â² |
| **ReLU** | max(0, x) | 1 if x > 0 else 0 |
| **Leaky ReLU** | max(Î±x, x) | 1 if x > 0 else Î± |
| **Softmax** | e<sup>xáµ¢</sup> / Î£ e<sup>xâ±¼</sup> | Used with cross-entropy |

---

## ğŸ§  Gradient Descent Variants

| Method | Update Rule | Description |
|--------|--------------|-------------|
| **Batch GD** | All samples per update | Stable but slow |
| **Stochastic GD** | One sample per update | Noisy but fast |
| **Mini-Batch GD** | Small subsets | Best trade-off |
| **Momentum** | Add velocity term | Faster convergence |
| **Adam** | Adaptive learning rates | Common in deep learning |

---

## ğŸ“Š Pseudocode Summary

```python
for epoch in range(epochs):
    # Forward pass
    A = forward(X, W, b)
    loss = compute_loss(A, Y)
    
    # Backward pass
    gradients = backprop(A, Y, W, b)
    
    # Weight update
    for l in layers:
        W[l] -= lr * gradients['dW'][l]
        b[l] -= lr * gradients['db'][l]
